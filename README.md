Disclaimer: README.md partially generated by AI

# Summit Auto Dealership Chatbot

A RAG (Retrieval-Augmented Generation) chatbot that answers questions about Summit Auto car dealership services using OpenAI's encoding and autoregressive LLMs.

## Overview

This chatbot provides accurate, grounded responses about dealership information by:
1. Loading dealership data from a JSON file
2. Creating text chunks with embeddings for semantic search
3. Retrieving relevant context based on user queries
4. Generating responses using OpenAI's chat models

## Features

- **Grounded Responses**: Answers only using provided dealership data
- **Semantic Search**: Uses cosine similarity on embeddings to find relevant context
- **Streaming Support**: Real-time response generation in the UI
- **Interactive Web UI**: Built with Gradio for easy interaction
- **Citation Tracking**: Shows which data chunks were used for each answer
- **Command-Line Testing**: Test queries directly from the terminal

## File Guide

- `app.py`: Main Gradio application and CLI entry point; handles loading data, embedding creation, retrieval, and response generation.
- `app_notebook.ipynb`: Jupyter notebook version of the app for iterative exploration or demos.
- `data.json`: Sample Summit Auto dealership knowledge base that powers retrieval (replace with your own).
- `requirements.txt`: Minimal dependencies needed when not using `uv`.
- `misc/`: JSON analysis and other files which are not needed for the project, but were used in development. Some development files were deleted.


## Setup
0. **Make sure you have Python => 3.10**

1. **Install dependencies**:
   if you have downloaded uv package manager at https://docs.astral.sh/uv/getting-started/installation/ 
   ```bash
   uv sync

   ```
   or create a venv and run 
   ```bash
   pip install -r requirements.txt
   ```

2. **Set up OpenAI API key**:
   Create a `.env` file in the project directory:
   ```
   OPENAI_API_KEY='INSERT_YOUR_API_KEY_HERE'
   ```

3. **Data file**:
   Ensure you have a `data.json` file with the dealership information structured as:
   ```json
   {
     "dealership": {
       "name": "Summit Auto",
       "meta": {...},
       "hours": {...},
       "locations": [...],
       "departments": [...],
       "services": [...],
       "faqs": [...],
       "policies": {...},
       "contact": {...},
       "booking": {...},
       "not_offered": [...]
     }
   }
   ```

## Usage

### Launch Interactive Web UI (Default)

Use the app_notebook.ipynb or follow the steps below for running the code in terminal. 

```bash
python app.py
```

This launches a Gradio interface with:
- Chat window for conversations
- Retrieved chunks display showing which data was used
- Clear button to reset conversation

### Command-Line Options

```bash
python app.py [OPTIONS]
```

**Options**:

- `--data PATH`: Path to JSON data file (default: `data.json`)
- `--model MODEL`: OpenAI model to use (default: `gpt-4.1-mini`)
- `--embedding_model MODEL`: Embedding model (default: `text-embedding-3-small`)
- `--k NUMBER`: Number of chunks to retrieve (default: `5`)
- `--no_stream`: Disable to test single questions without UI (default: enabled)
- `--question TEXT`: Ask a specific question in CLI mode, works if --no_stream flag is enabled, which disables streaming


## How It Works

### 1. Data Chunking
The `create_chunks()` function breaks down the JSON data into small, searchable snippets:
- Dealership metadata (name, hours, departments)
- Location details (address, phone, parking)
- Services (descriptions, prices, duration)
- FAQs (question-answer pairs)
- Policies and contact information

Each chunk includes:
- `id`: Unique identifier
- `snippet`: Text content
- `path`: JSON path reference
- `embedding`: Vector representation

### 2. Semantic Search
When a user asks a question:
1. The question is converted to an embedding vector
2. Cosine similarity is calculated against all chunk embeddings
3. Top K most similar chunks are retrieved as context

### 3. Response Generation
The system:
1. Injects retrieved chunks into the system prompt
2. Maintains conversation history for context
4. Streams the response back to the user in real-time


## Limitations

- Responses are limited to information in the JSON file
- Does not have real-time availability or booking capabilities (non-agentic)
- Cannot access external databases or systems

## Future Improvements

- [ ] Add conversation memory persistence
- [ ] Implement re-ranking with a seperate LLM for better context selection (not useful now, because our data is very simple)
- [ ] Include confidence scores for responses